{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ec5f5a",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This notebook will:\n",
    "#### extrac the embeddings for each model (baseline vs fine tuned) \n",
    "#### train a shallow model (xgboost ) \n",
    "#### test the validation set accuracy against the shallow model results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3580d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from esm import FastaBatchedDataset, pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2098a31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EsmTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mEsmTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/esm2_t6_8M_UR50D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m EsmModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/esm2_t6_8M_UR50D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_embeddings\u001b[39m(model, fasta_file,caslabel, output_dir, tokens_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1022\u001b[39m,repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m33\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EsmTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "def extract_embeddings(model, fasta_file,caslabel, output_dir, tokens_per_batch=4096, seq_length=1022,repr_layers=[33]):\n",
    "    from transformers import EsmTokenizer, EsmModel\n",
    "    import torch\n",
    "    # read the fasta file and extract sequences\n",
    "\n",
    "    \n",
    "    seqs =[seq]\n",
    "    inputs = tokenizer(seqs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    x = last_hidden_states.detach()\n",
    "    x =x.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddingsx(model_name, fasta_file,caslabel, output_dir, tokens_per_batch=4096, seq_length=1022,repr_layers=[33]):\n",
    "    \n",
    "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda('cuda:1')\n",
    "    \n",
    "#     caslist = ['cas1','cas2','cas3','cas4','cas10','cas12','cas14','cas9','cas13a', 'cas13b', 'cas13c','cas13d']\n",
    "    dataset = FastaBatchedDataset.from_file(fasta_file)\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=alphabet.get_batch_converter(seq_length), \n",
    "        batch_sampler=batches\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "\n",
    "            print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                toks = toks.to(device=\"cuda:1\", non_blocking=True)\n",
    "\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n",
    "            \n",
    "\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]\n",
    "                \n",
    "                filename = output_dir / f\"{entry_id}.pt\"\n",
    "                truncate_len = min(seq_length, len(strs[i]))\n",
    "\n",
    "                result = {\"entry_id\": entry_id}\n",
    "                caslabel = \"\"\n",
    "                for word in caslist:\n",
    "                    if word.lower() in label.lower():\n",
    "                        caslabel = word.lower()\n",
    "                    \n",
    "                result['label'] = caslabel\n",
    "                result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "                \n",
    "                \n",
    "                \n",
    "                torch.save(result, filename)\n",
    "#                 if i >1:\n",
    "#                     return(result)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7740e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings0(model_name, fasta_file, caslabel, output_dir, tokens_per_batch=4096, seq_length=1022, repr_layers=[33]):\n",
    "    from Bio import SeqIO\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Load your model and alphabet (assuming a custom function or similar exists)\n",
    "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Setup model for multiple GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = FastaBatchedDataset.from_file(fasta_file)\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    data_loader = DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=alphabet.get_batch_converter(seq_length), \n",
    "        batch_sampler=batches\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "            print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n",
    "\n",
    "            toks = toks.cuda()  # Ensure tokens are on GPU\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n",
    "            \n",
    "            # Process outputs\n",
    "            logits = out[\"logits\"].cpu()\n",
    "            representations = {layer: t.cpu() for layer, t in out[\"representations\"].items()}\n",
    "\n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]\n",
    "                filename = output_dir / f\"{entry_id}.pt\"\n",
    "                truncate_len = min(seq_length, len(strs[i]))\n",
    "\n",
    "                result = {\"entry_id\": entry_id, \"label\": caslabel}\n",
    "                result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "                \n",
    "                \n",
    "                torch.save(result, filename)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "caslist = ['cas1','cas2','cas3','cas4','cas5','cas6','cas7','cas8','cas9','cas10','cas11','cas12','cas13']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'esm2_t33_650M_UR50D'\n",
    "model_name = 'esm2_t6_8M_UR50D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39150149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fc36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb98e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6dcf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_layer_number = 6 # for the 8 m model, for 650 use 33 \n",
    "\n",
    "for cas in caslist: \n",
    "    \n",
    "    casfolder = f\"/home/salaris/protein_model/data/{cas}/\"\n",
    "    \n",
    "    training_fasta_file = pathlib.Path(casfolder + cas + '_training.fasta')\n",
    "    validation_fasta_file = pathlib.Path(casfolder + cas + '_validation.fasta')\n",
    "    \n",
    "    training_embedding_folder = pathlib.Path(casfolder  + \"_\" +model_name + \"_\" + 'embeddings/' +  'training/')\n",
    "    validation_embedding_folder = pathlib.Path(casfolder  + \"_\" +model_name + \"_\" + 'embeddings/' +   'validation/')\n",
    "    print(training_embedding_folder, validation_embedding_folder)\n",
    "    print(training_fasta_file, validation_fasta_file)\n",
    "    extract_embeddings(model_name, \n",
    "                       fasta_file= training_fasta_file,caslabel= cas, \n",
    "                       output_dir= training_embedding_folder, tokens_per_batch=2048 * 2, seq_length=1022,repr_layers=[rep_layer_number])\n",
    "\n",
    "    extract_embeddings(model_name, \n",
    "                       fasta_file= validation_fasta_file,caslabel= cas, \n",
    "                       output_dir= validation_embedding_folder, tokens_per_batch=2048 * 2 , seq_length=1022,repr_layers=[rep_layer_number])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13faa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
